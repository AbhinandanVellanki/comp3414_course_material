{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskedRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzWBZZLorTH84pN62Lq9/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesley34/comp3414_course_material/blob/master/ch11_python_Tensorflow_2.0/MaskedRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3odxy705Gf_b",
        "colab_type": "text"
      },
      "source": [
        "## on process "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWyB8RXZtlR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## HYPERPARAMETER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWI00U6Pt8zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import datetime\n",
        "import re\n",
        "import math\n",
        "import logging\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import skimage.color\n",
        "import skimage.io\n",
        "import skimage.transform\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers as KL\n",
        "from tensorflow.keras import models as KM\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBEbcz4Wuzw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "edfb8eb6-6897-43b1-d51e-eb75eb05a376"
      },
      "source": [
        "#TODO\n",
        "deltas = np.array([[[1,2,3,4],[2,2,3,4]],[[5,6,7,8],[2,6,7,8]]])\n",
        "ix = np.array([[1,0],[1,0]])\n",
        "for data,i in zip(deltas,ix):\n",
        "  print(tf.gather(data,i).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 2 3 4]\n",
            " [1 2 3 4]]\n",
            "[[2 6 7 8]\n",
            " [5 6 7 8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adfen8KTu-j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SETUP IMAGE\n",
        "IMAGE_MIN_DIM = 800\n",
        "IMAGE_MAX_DIM = 1024\n",
        "IMAGE_DIM = IMAGE_MAX_DIM\n",
        "IMAGE_RESIZE_MODE = \"square\"\n",
        "IMAGE_MIN_SCALE = 0\n",
        "\n",
        "# SETUP BACKBONE NETWORK\n",
        "BACKBONE = \"resnet101\"\n",
        "BACKBONE_STRIDES = [4,8,16,32,64]\n",
        "\n",
        "# SETUP RPN\n",
        "RPN_ANCHOR_STRIDE = 1                #stride of rpn\n",
        "RPN_ANCHOR_SCALES = (32,64,128,256,512)       #different scales for rpn upsampling\n",
        "RPN_ANCHOR_RATIO = [0.5,1,2]             #aspect ratio of rpn achor\n",
        "RPN_TRAIN_ANCHORS_PER_IMAGE = 256           #number of anchors to be chosen\n",
        "\n",
        "# SETUP ROI\n",
        "TRAIN_ROI_PER_IMAGE = 200             #use how many roi to fpn layer\n",
        "ROI_POSITIVE_RATIO = 0.33             #use how many positive roi ratio into fpn\n",
        "\n",
        "# SET UP NMS\n",
        "POST_NMS_ROIS_TRAINING = 2000\n",
        "POST_NMS_ROIS_INFERENCE = 1000\n",
        "RPN_MNS_THRESHOLD = 0.3\n",
        "FPN_FREATURE = 256\n",
        "DETECTION_MAX_INSTANCE = 100 \n",
        "\n",
        "MAX_GT_INSTANCE = 100\n",
        "\n",
        "DETECTION_MIN_CONFIDENCE = 0.7\n",
        "\n",
        "DETECTION_MNMS_THRESHOLD = 0.3\n",
        "\n",
        "# ROI pooling setup\n",
        "POOL_SIZE = 7\n",
        "MASK_POOL_SIZE = 14\n",
        "MASK_SHAPE = [28,28]\n",
        "\n",
        "RPN_BBOX_STD_DEV = np.array([0.1,0.1,0.2,0.2])\n",
        "BBOX_STD_DEV = np.array([0.1,0.1,0.2,0.2])\n",
        "\n",
        "USE_MINI_MASK = True\n",
        "MINI_MASK_SHAPE = (56,56) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R53UpxGQ1x42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw1JSFitzUMN",
        "colab_type": "text"
      },
      "source": [
        "## Section 1 : Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hCbvilI6Ytz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## construction of resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_WkiDPT1zS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_backbone_shapes(image_shape):\n",
        "  return_shape = [[int(math.ceil(image_shape[0]/stride)),int(math.ceil(image_shape[1]/stride))] for stride in BACKBONE_STRIDES]\n",
        "  return np.array(return_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz7KABLV2H96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resnet identity_block\n",
        "def identity_block(input_tensor,kernel_size,filters,stage,block,use_bias=True,train_bn=True):\n",
        "  nb_filter1,nb_filter2,nb_filter3 = filters\n",
        "  conv_name_base = \"res\" + str(stage) + block + \"_branch\" #conv layer name\n",
        "  bn_name_base = \"bn\" + str(stage) + block + \"_branch\" #bn layer name\n",
        "\n",
        "  x = KL.Conv2D(nb_filter1,(1,1),name=conv_name_base + '2a',use_bias=use_bias)(input_tensor) #output \n",
        "  x = KL.BatchNormalization(name=bn_name_base + '2a')(x,training=train_bn)\n",
        "  x = KL.Activation('relu')\n",
        "\n",
        "  x = KL.Conv2D(nb_filter2,(kernel_size,kernel_size),padding=\"same\",name=conv_name_base+\"2b\",use_bias=use_bias)(x)\n",
        "  x = KL.BatchNormalization(name=bn_name_base + '2b')(x,training=train_bn)\n",
        "  x = KL.Activation('relu')\n",
        "\n",
        "  x = KL.Conv2D(nb_filter3,(1,1),name=conv_name_base + '2c',use_bias=use_bias)(input_tensor)\n",
        "  x = KL.BatchNormalization(name=bn_name_base + '2c')(x,training=train_bn)\n",
        "  \n",
        "  x = KL.add()([x,input_tensor])\n",
        "  x = KL.Activation('relu',name=\"res\"+str(stage)+block+\"_out\")(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaCCTkw06d75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## resnet conv_block\n",
        "def conv_block(input_tensor,kernel_size,filters,stage,block,strides=(2,2),use_bias=True,train_bn=True):\n",
        "  nb_filter1,nb_filter2,nb_filter3 = filters\n",
        "  conv_name_base = \"res\" + str(stage) + block + \"_branch\" #conv layer name\n",
        "  bn_name_base = \"bn\" + str(stage) + block + \"_branch\" #bn layer name\n",
        "\n",
        "  # downsampling , strides = (2,2) => x = concat(x, shortcut)\n",
        "  # shortcut is done by conv with filter3 and stride => batchNorm => shortcut\n",
        "  # x is done as identity block with initial strides = (2,2) a.k.a downsampling\n",
        "  # first layer \n",
        "\n",
        "  x = KL.Conv2D(nb_filter1,(1,1),strides=strides ,name=conv_name_base + '2a',use_bias=use_bias)(input_tensor) #output \n",
        "  x = KL.BatchNormalization(name=bn_name_base + '2a')(x,training=train_bn)\n",
        "  x = KL.Activation('relu')\n",
        "\n",
        "  x = KL.Conv2D(nb_filter2,(kernel_size,kernel_size),padding=\"same\",name=conv_name_base+\"2b\",use_bias=use_bias)(x)\n",
        "  x = KL.BatchNormalization(name=bn_name_base + '2b')(x,training=train_bn)\n",
        "  x = KL.Activation('relu')\n",
        "\n",
        "  x = KL.Conv2D(nb_filter3,(1,1),name=conv_name_base + '2c',use_bias=use_bias)(input_tensor)\n",
        "  x = KL.BatchNormalization(name=bn_name_base + '2c')(x,training=train_bn)\n",
        "  \n",
        "  shortcut = KL.Conv2D(nb_filter3,(1,1),strides=strides,name=conv_name_base + \"1\",use_bias=use_bias)(input_tensor)\n",
        "  shortcut = KL.BatchNormalization(name=bn_name_base + '1')(shortcut,training=train_bn)\n",
        "\n",
        "  x = KL.add()([x,shortcut])\n",
        "  x = KL.Activation('relu',name=\"res\"+str(stage)+block+\"_out\")(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-F5in3o6rQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## support resnet 50 / 101 , statement support for stage 5 feature map output\n",
        "\"\"\"\n",
        "@param\n",
        "FORMAT: param_name : type || meaning\n",
        "-----------------------------------------------------------------\n",
        "input_image : tensor || image that feed to network\n",
        "architecture : string || choose from resnet 50 / resnet 101\n",
        "stage 5 : bool || if you want output stage5\n",
        "train_bn : bool || if you want to train for batch_normalization\n",
        "-----------------------------------------------------------------\n",
        "@process description\n",
        "it builds a resnet-FPN and return you C1,C2,C3,C4,C5\n",
        "-----------------------------------------------------------------\n",
        "@output\n",
        "list of Feature with different size \n",
        "[C1,C2,C3,C4,C5]\n",
        "-----------------------------------------------------------------\n",
        "\"\"\"\n",
        "def resnet_graph(input_image,architecture,stage5=False,train_bn=True):\n",
        "  # check architecture\n",
        "  assert architecture in {\"resnet50\",\"resnet101\"}\n",
        "  \"\"\"------------------------------------------------------------------------------------------\"\"\"\n",
        "\n",
        "  # first feature layer\n",
        "  x = KL.ZeroPadding((3,3))(input_image)\n",
        "  x = KL.Conv2D(64,(7,7),strides=(2,2,),name=\"conv1\",use_bias=True)(x)\n",
        "  x = KL.BatchNormalization(name=\"bn_conv1\")(x,training=train_bn)\n",
        "  x = KL.Activation(\"relu\")(x)\n",
        "  C1 = x = KL.MaxPooling2D((3,3),strides=(2,2),padding=\"same\")(x)\n",
        "\n",
        "  \"\"\"------------------------------------------------------------------------------------------\"\"\"\n",
        "  # second feature layer\n",
        "  x = conv_block(x,3,[64,64,256],stage=2,block=\"a\",train_bn=train_bn)\n",
        "  x = identity_block(x,3,[64,64,256],stage=2,block=\"b\",train_bn=train_bn)\n",
        "  C2 = x = identity_block(x,3,[64,64,256],stage=2,block=\"c\",train_bn=train_bn)\n",
        "\n",
        "\n",
        "  \"\"\"------------------------------------------------------------------------------------------\"\"\"\n",
        "  # thrid feature layer\n",
        "  x = conv_block(x,3,[128,128,512],stage=3,block=\"a\",train_bn=train_bn)\n",
        "  x = identity_block(x,3,[128,128,512],stage=3,block=\"b\",train_bn=train_bn)\n",
        "  x = identity_block(x,3,[128,128,512],stage=3,block=\"b\",train_bn=train_bn)\n",
        "  C3 = x = identity_block(x,3,[128,128,512],stage=3,block=\"c\",train_bn=train_bn)\n",
        "\n",
        "  \"\"\"------------------------------------------------------------------------------------------\"\"\"\n",
        "  # forth feature layer \n",
        "  x = conv_block(x,3,[256,256,1024],stage=4,block=\"a\",train_bn=train_bn)\n",
        "  block_count = {\"resnet50\":5,\"resnet101\":222}[architecture] #get number from dictionary depends on your architecture\n",
        "  for i in range(block_count):\n",
        "    x = identity_block(x,3,[256,256,1024],stage=4,block=chr(98+1),train_bn=train_bn)\n",
        "  C4 = x\n",
        "\n",
        "  \"\"\"------------------------------------------------------------------------------------------\"\"\"\n",
        "  # fifth feature layer\n",
        "  if stage5:\n",
        "    x = conv_block(x,3,[512,512,2048],stage=5,block=\"a\",train_bn=train_bn)\n",
        "    x = identity_block(x,3,[512,512,2048],stage=5,block=\"b\",train_bn=train_bn)\n",
        "    C5 = x = identity_block(x,3,[512,512,2048],stage=5,block=\"c\",train_bn=train_bn)\n",
        "  else:\n",
        "    C5 = None\n",
        "  return [C1,C2,C3,C4,C5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW3HIqyBzz4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQKlSld4zz7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciLcRSFzyz-_",
        "colab_type": "text"
      },
      "source": [
        "## Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omwzBSuzy0Ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Image mean RGB\n",
        "MEAN_PIXEL = np.array([123.7,116.8,103.9])\n",
        "\n",
        "def mold_image(images):\n",
        "  return images.astype(np.float32) - MEAN_PIXEL\n",
        "def unmodel_image(normalized_images):\n",
        "  return (normalized_images + MEAN_PIXEL).astype(np.uint8)\n",
        "def resize_image(image,min_dim=None,max_dim=None,min_scale=None,mode=\"square\"):\n",
        "  image_dtype = image.dtype\n",
        "  h,w = image.shape[:2]\n",
        "  window = (0,0,h,w)\n",
        "  scale = 1\n",
        "  padding = [(0,0),(0,0),(0,0)]\n",
        "  crop = None\n",
        "  if mode == \"none\":\n",
        "    return image,window,scale,padding,crop\n",
        "  if min_dim:\n",
        "    scale = max(1,min_dim/min(h,w))\n",
        "  if min_scale and scale < min_scale:\n",
        "    scale = min_scale\n",
        "\n",
        "  # if exceed max dim\n",
        "  if max_dim and mode == \"square\":\n",
        "    image_max = max(h,w)\n",
        "    if round(image_max * scale) > max_dim:\n",
        "      scale = max_dim / image_max\n",
        "  \n",
        "  if scale != 1:\n",
        "    image = skimage.transform.resize(\n",
        "        image,(round(h*scale),round(w*scale)),\n",
        "        order=1,mode=\"constant\",preserve_range=True\n",
        "    )\n",
        "  \n",
        "  if mode == \"square\":\n",
        "    # get new height and width\n",
        "    h,w = image.shape[:2]\n",
        "    top_pad = (max_dim-h)//2\n",
        "    bottom_pad = min_dim-h-top_pad\n",
        "    left_pad = (max_dim-w)//2\n",
        "    right_pad = max_dim-w-left_pad\n",
        "    padding = [(top_pad,bottom_pad),(left_pad,right_pad),(0,0)]\n",
        "    image = np.pad(image,padding,mode=\"constant\",constant_values=0)\n",
        "    window = (top_pad,left_pad,h+top_pad,w+left_pad)\n",
        "  elif mode == \"pad64\":\n",
        "    h,w = image.shape[:2]\n",
        "    assert min_dim % 64 == 0, \"Minimum dimension must be multiple of 64\"\n",
        "    if h%64>0:\n",
        "      max_h = h-(h%64)+64\n",
        "      top_pad = (max_h-h)//2\n",
        "      bottom_pad = max_h - top_pad - h\n",
        "    else:\n",
        "      top_pad = bottom_pad = 0\n",
        "    if w%64>0:\n",
        "      max_w = w-(w%64)+64\n",
        "      left_pad = (max_w-w)//2\n",
        "      right_pad = max_w-w-right_pad\n",
        "    else:\n",
        "      left_pad = right_pad = 0\n",
        "    padding = [(top_pad,bottom_pad),(left_pad,right_pad),(0,0)]\n",
        "    image = np.pad(image,padding,mode=\"constant\",constant_values=0)\n",
        "    window = (top_pad,left_pad,top_pad+h,left_pad+w)\n",
        "  elif mode == \"crop\":\n",
        "    # random crop\n",
        "    h,w = image.shape[:2]\n",
        "    y = random.randint(0,(h-min_dim))\n",
        "    x = random.randint(0,(w-min_dim))\n",
        "    crop = (y,x,min_dim,min_dim)\n",
        "    image = image[y:y+min_dim,x:x+min_dim]\n",
        "    window = (0,0,min_dim,min_dim)\n",
        "  else:\n",
        "    raise Exception(\"Mode {} not support\".format(mode))\n",
        "  return image.astype(image_dtype),window,scale,padding,crop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_-2niXy4Mx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compose_image_meta(image_id,original_image_shape,image_shape,window,scale,active_class_ids):\n",
        "  meta = np.array([image_id]+         # 1\n",
        "            list(original_image_shape) # 3\n",
        "            list(image_shape)+    # 3\n",
        "            list(window)+       # 4\n",
        "            [scale]+         # 1\n",
        "            list(active_class_ids)  # num_class size\n",
        "      )\n",
        "  return meta\n",
        "\n",
        "def log(text,array=None):\n",
        "  if array is Not None:\n",
        "    text = text.ljust(25)\n",
        "    text += (\"shape: {:20} min: {:10.5f} max {:10.5f} {}\".format(\n",
        "        str(array.shape),\n",
        "        array.min() if array.size else \"\",\n",
        "        array.max() if array.size else \"\",\n",
        "        array.dtype\n",
        "    ))\n",
        "  print(text)\n",
        "\n",
        "def run_graph(MaskRCNNobj,images,outputs,batch_size,image_metas=None):\n",
        "  model = MaskRCNNobj.keras_model\n",
        "  outputs = OrderedDict(outputs)\n",
        "  for o in outputs.values()\n",
        "    assert o is not None\n",
        "\n",
        "  inputs = model.inputs\n",
        "\n",
        "  kf = K.function(model.inputs,list(outputs.values()))\n",
        "\n",
        "  if image_metas is None:\n",
        "    molded_images,images_metas,_ = MaskRCNNobj.mold_inputs(images) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2-lXgAaz0Q9",
        "colab_type": "text"
      },
      "source": [
        "## Section2 : RPN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1N3QKQFyy0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zMJrAVzz0Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "functon rpn_graph\n",
        "@ param\n",
        "feature_map : output layer from resnet-FPN\n",
        "anchors_per_location: int , how much anchors you need for 1 location box\n",
        "anchor_stride, int , the stride for anchor to conv the photo\n",
        "@ output\n",
        "logits,class,bbox \n",
        "\"\"\"\n",
        "def rpn_graph(feature_map,anchors_per_location,anchor_stride):\n",
        "  shared = KL.Conv2D(512,(3,3),padding=\"same\",activation=\"relu\",strides=anchor_stride,name=\"rpn_conv_shared\")(feature_map)\n",
        "\n",
        "  # step 1 : calculate the score of anchors [batch_size,height,width,anchors_per_location * 2] => *2 helps to reshape as 2 classes [positive and negative]\n",
        "  # scale up channel output to 2*anchors_per_location\n",
        "  x = KL.Conv2D(2*anchors_per_location,(1,1),padding=\"valid\",activation=\"linear\",name=\"rpn_class_raw\")(shared)\n",
        "\n",
        "  # feature map extention to [batch,anchors_location]\n",
        "  rpn_class_logits = KL.lambda(lambda t:tf.reshape(t,[shape(t)[0],-1,2]))(x) ## batch_size,width*height*anchor_per_location,2]\n",
        "\n",
        "  # calculate the feature map's score with respect to positive and negative\n",
        "  rpn_probs = KL.Activation(\"softmax\",name=\"rpn_class_xxx\")(rpn_class_logits) # [batch_size,width*height*anchor_per_location,2] => positive means front-side else back-side\n",
        "\n",
        "  # step 2: calculate the bbox\n",
        "  x = KL.Conv2D(4*anchors_per_location,(1,1),padding=\"valid\",activation=\"linear\",name=\"rpn_bbox_pred\")(shared)\n",
        "\n",
        "  rpn_bbox = KL.Lambda(lambda t : tf.reshape(t,[t.shape[0],-1,4]))(x)\n",
        "\n",
        "  return [rpn_class_logits,rpn_probs,rpn_bbox]\n",
        "\"\"\"\n",
        "function build_rpn_model\n",
        "@ param\n",
        "depth : int , channel-in, say rgb means channel is 3, greytone means 1\n",
        "\"\"\"\n",
        "def build_rpn_model(anchor_stride,anchors_per_location,depth): \n",
        "\n",
        "  input_feature_map = KL.Input(shape=[None,None,depth],name=\"input_rpn_feature_map\")\n",
        "  outputs = rpn_graph(input_feature_map,anchors_per_location,anchor_stride)\n",
        "  model = KM.Model(inputs=input_feature_map,outputs=outputs,name=\"rpn_model\")\n",
        "  return model\n",
        "\n",
        "\n",
        "def apply_box_deltas_graph(boxes,deltas): #[batches,[y1,x1,y2,x2]],[batches,[dy,dx,log(dh),log(dw)]]\n",
        "\n",
        "  height = boxes[:,2] - boxes[:,0]\n",
        "  width = boxes[:,3] - boxes[:,1]\n",
        "  center_y = boxes[:,0] + 0.5 * height\n",
        "  center_x = boxes[:,1] + 0.5 * width\n",
        "  \n",
        "  # adding with bias \n",
        "  center_y += delta[:,0] * height\n",
        "  center_x += delta[:,1] * width\n",
        "  height *= tf.exp(delta[:,2])\n",
        "  width *= tf.exp(delta[:,3])\n",
        "\n",
        "  #make new y1,x1,y2,x2\n",
        "  y1 = center_y - 0.5*height\n",
        "  y2 = center_y + 0.5*height\n",
        "  x1 = center_x - 0.5*width\n",
        "  x2 = center-x + 0.5*width\n",
        "\n",
        "  result = tf.stack([y1,x1,y2,x2],axis=1,name=\"apply_box_delta_out\")\n",
        "  return result\n",
        "\n",
        "def clip_boxes_graph(boxes,window): # [batches,[y1,x1,y2,x2]],[wy1,wx1,wy2,wx2]\n",
        "  wy1,wx1,wy2,wx2 = tf.split(window,4)\n",
        "  y1,x1,y2,x2 = tf.split(boxes,4,axis=1)\n",
        "  # cutting\n",
        "  y1 = tf.maximum(tf.minimum(y1,wy2),wy1)\n",
        "  y2 = tf.maximum(tf.minimum(y2,wy2),wy1)\n",
        "  x1 = tf.maximum(tf.minimum(x1,wx2),wx1)\n",
        "  x2 = tf.maximum(tf.minimum(x2,wy2),wx1)\n",
        "\n",
        "  clipped = tf.concat([y1,y2,x1,x2],axis=1,name=\"clipped_box\")\n",
        "  clipped.set_shape((clipped.shape[0],4))\n",
        "  return clipped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuS09bi2yx7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_OxhgmGz0et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProposalLayer(tf.keras.layers.Layers):\n",
        "\n",
        "  def __init__(self,proposal_count,nms_threshold,batch_size,**kwargs):\n",
        "    super(ProposalLayer,self).__init__(**kwargs)\n",
        "    self.proposal_count = proposal_count\n",
        "    self.nms_threshold = mns_threshold\n",
        "    self.batch_size = batch_size\n",
        "  \n",
        "  '''\n",
        "  @param\n",
        "  rpn_probs = [batch,num_anchros,2] #inputs[0]\n",
        "  rpn_bbox = [batch,num_anchors,(dy,dx,log(dh),log(dw))]  #inputs[1]\n",
        "  anchors = [batch,(y1,x1,y2,x2)]  #inputs[2]\n",
        "  '''\n",
        "  def call(self,inputs):\n",
        "    scores = inputs[0][:,:,1] #batch,num_anchors,1\n",
        "    deltas = inputs[1]\n",
        "    deltas = deltas * np.reshape(mask_rcnn_model.RPN_BBOX_STD_DEV,[1,1,4])\n",
        "    anchors = inputs[2] \n",
        "\n",
        "    pre_nms_limit = tf.minimum(6000,tf.shape(anchors)[1]) # get 6000 or less\n",
        "    ix = tf.nn.top_k(scores,pre_nms_limit,sorted=True,name=\"top_anchors\").indices # base on score , get best 6000\n",
        "    scores = utils.batch_slice()\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y1ijY8zz0hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTQ-7JDaz0jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec_3xegbyAvl",
        "colab_type": "text"
      },
      "source": [
        "## Build all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htid8Rl76uxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskRCNN():\n",
        "  def __init__(self,mode,model_dir,num_class,batch_size):\n",
        "    assert mode = \"inference\"\n",
        "\n",
        "    self.mode = mode\n",
        "    self.num_class = num_class\n",
        "    self.batch_size = batch_size\n",
        "    self.model_dir = model_dir\n",
        "    self.set_log_dir()\n",
        "    self.keras_model = self.build(mode=mode)\n",
        "\n",
        "  # image_preprcoessing\n",
        "  def mold_inputs(self,images):\n",
        "    moleded_image = []\n",
        "    image_metas = []\n",
        "    windows = []\n",
        "    for image in images:\n",
        "\n",
        "      #window is scaled position\n",
        "      #scale is aspect ratioh\n",
        "      # TODO\n",
        "      pass\n",
        "  def build(self,mode):\n",
        "    h,w = IMAGE_DIM , IMAGE_DIM\n",
        "    if h / 2**6 != int(h / 2**6 ) or  w / 2**6 != int(w / 2**6 ):\n",
        "      raise Exception(\"Image size must be divisible by 64\")\n",
        "\n",
        "    input_image = KL.Input(shape=[None,None,3],name=\"input_image\")\n",
        "\n",
        "    input_image_meta = KL>Input(shape=[img_meta_size],name=\"input_image_meta\")\n",
        "\n",
        "    if mode == \"inference\":\n",
        "      input_anchors = KL.Input(shape=[None,4],name=\"input_anchors\")\n",
        "\n",
        "    # SECTION 1 : build resnet-FPN\n",
        "\n",
        "    # step 1 build resnet\n",
        "    # output from resnet upsampling\n",
        "    _,C2,C3,C4,C5 = resnet_graph(input_image,BACKBONE,stage5=True,train_bn=False)\n",
        "\n",
        "    # step 2 build FPN\n",
        "    P5 = KL.Conv2D(256,(1,1),name=\"fpn_c5p5\")(C5)\n",
        "    P4 = KL.Add(name=\"fpn_p4add\")([KL.UpSampling2D(size=(2,2),name=\"fpn_p5upsampled\")(P5),\n",
        "                                   KL.Conv2D(256,(1,1),name=\"fpn_c4p4\")\n",
        "                                   ])\n",
        "    P3 = KL.add(name=\"fpn_p3add\")([KL.UpSampling2D(size=(2,2),name=\"fpn_p4upsampled\")(P4),\n",
        "                                   KL.Conv2D(256,(1,1),name=\"fpn_c3p3\")\n",
        "                                   ])\n",
        "    P2 = KL.add(name=\"fpn_p2add\")([KL.UpSampling2D(size=(2,2),name=\"fpn_p3upsampled\")(P3),\n",
        "                                   KL.Conv2D(256,(1,1),name=\"fpn_c2p2\")\n",
        "                                   ])\n",
        "    \n",
        "    # conv to all P\n",
        "\n",
        "    P2 = KL.Conv2D(FPN_FREATURE,(3,3),padding=\"same\",name=\"fpn_p2\")(P2)\n",
        "    P3 = KL.Conv2D(FPN_FREATURE,(3,3),padding=\"same\",name=\"fpn_p3\")(P3)\n",
        "    P4 = KL.Conv2D(FPN_FREATURE,(3,3),padding=\"same\",name=\"fpn_p4\")(P4)\n",
        "    P5 = KL.Conv2D(FPN_FREATURE,(3,3),padding=\"same\",name=\"fpn_p5\")(P5)\n",
        "    P6 = KL.MaxPooling2D(pool_size=(1,1),strides=2,name=\"fpn_p6\")(P6)\n",
        "\n",
        "    rpn_feature_maps = [P2,P3,P4,P5,P6]\n",
        "    mrcnn_feature_maps = [P2,P3,P4,P5] \n",
        "    #--------------------------------------------------------------------------------------------------------------------------\n",
        "    # END OF RESNET-FPN\n",
        "\n",
        "    # SECTION2 : build forward rpn\n",
        "    # define rpn Regional proposal network\n",
        "    rpn = build_rpn_model(RPN_ANCHOR_STRIDE,len(RPN_ANCHOR_STRIDE),FPN_FREATURE)\n",
        "\n",
        "    layer_outputs = []\n",
        "    for p in rpn_feature_maps:\n",
        "      #use all features in rpn_feature maps to feed the rpn\n",
        "      layer_outputs.append(rpn([p]))\n",
        "    \n",
        "    output_names = [\"rpn_class_logits\",\"rpn_class\",\"rpn_bbox\"]\n",
        "    # [[rpn_class_logits1,rpn_class1,rpn_bbox1],[rpn_class_logits2,rpn_class2,rpn_bbox2] becomes [[rpn_class_logits1,rpn_class_logits2],[rpn_class1,rpn_class2],[rpn_bbox1,rpn_bbox2]]\n",
        "    outputs = list(zip(*layer_outputs)) #any size it can be due to the * so [[1,2,3],[4,5,6]] can be resize to [[1,4],[2,5],[3,6]] after zip(outputs,output_names)\n",
        "    # outputs shape follows outputs_names !!!!!\n",
        "    outputs = [KL.Concatenate(axis=1,name=n)(list(o)) for o,n in zip(outputs,output_names)]\n",
        "    rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
        "\n",
        "    proposal_count = POST_NMS_ROIS_TRAINING if mode == \"inference\" else POST_NMS_ROIS_INFERENCE\n",
        "\n",
        "    if mode == \"inference\":\n",
        "      anchors = input_anchors\n",
        "    \n",
        "    rpn_rois = ProposalLayer(proposal_count=proposal_count,mns_threshold=RPN_MNS_THRESHOLD,batch_size=self.batch_size,name=\"ROI\")([rpn_class,rpn_bbox,anchors])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFZmt0CD6uzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WriQ4GUg6vMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}